# -*- coding: utf-8 -*-
"""HW3 Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndMA2j7Fwb-TxRwGsepUrhJgso6gh4-T
"""

# Commented out IPython magic to ensure Python compatibility.
# CS 3190 - HW3 - Question 3
# Jessika Jimenez

import numpy as np 
from scipy import linalg as LA
import pandas as pd
import random 
import math
import matplotlib as mpl
import matplotlib.pyplot as plt 
# %matplotlib inline
from google.colab import drive
drive.mount("/content/gdrive")

# import & save values from x.csv
def read_file(file_x):
  x_data = []
  with open(file_x, "r") as f:
    for line in f:
      item = line.strip().split(",")
      x_data.append(np.array(item))
  return x_data
x_data = read_file('/content/gdrive/My Drive/University of Utah/2022 Fall/CS 3190/x.csv')
x = np.zeros(len(x_data))
for i in range(len(x_data)):
  x[i] = float(x_data[i])
#print(x, '\n')

# import & save values from y.csv
def read_file(file_y):
  y_data = []
  with open(file_y, "r") as f:
    for line in f:
      item = line.strip().split(",")
      y_data.append(np.array(item))
  return y_data
y_data = read_file('/content/gdrive/My Drive/University of Utah/2022 Fall/CS 3190/y.csv')
y = np.zeros(len(y_data))
for i in range(len(y_data)):
  y[i] = float(y_data[i])
#print(y, '\n')


#-------PROBLEM 3-------
# 3.(a) - Run gradient descent on f1 with starting point (x, y) = (0, 0), 
#         T = 30 steps and Œ≥ = .01
# expanded f1 function
def func_f1(x, y):
  return ((x ** 2) - (6*x) + (3*y ** 2) + (6*y) + 12)

# gradient of function f1
def func_grad(vx, vy):
  dfdx = 2*vx
  dfdy = 6*vy + 6
  return np.array([dfdx, dfdy])

# prep for plot
xlist = np.linspace(-0.5, 0.5, 26)
ylist = np.linspace(-1, 0.5, 26)
x_, y_ = np.meshgrid(xlist, ylist)
z_ = func_f1(x_, y_)
lev = np.linspace(0, 20, 21)

# initialize gradient descent at (0,0), 30 iterations, ùõæ=0.01
v_init = np.array([0, 0])
num_iter = 30
values = np.zeros([num_iter, 2])
values[0,:] = v_init
v = v_init
gamma = 0.01

# gradient descent algorithm
for i in range(1,num_iter):
  v = v - gamma * func_grad(v[0], v[1])
  values[i,:] = v

# plot
plt.contour(x_, y_, z_, levels = lev)
plt.plot(values[:,0], values[:,1], 'r-')
plt.plot(values[:,0], values[:,1], 'bo')
grad_norm = LA.norm(func_grad(v[0], v[1]))
title = "gamma %0.02f | final grad %0.3f" % (gamma, grad_norm)
plt.title(title)
plt.show()
print('function value, gradient, and 2-norm of the gradient \n')
# ******TODO - FIXME(?)



# 3.(b) - Run gradient descent on f1 with starting point (x, y) = (10, 10), 
#         T = 100 steps and Œ≥ = .03
# prep for plot
xxlist = np.linspace(0, 5, 26)
yylist = np.linspace(-2.5, 2.5, 26)
xx_, yy_ = np.meshgrid(xxlist, yylist)
zz_ = func_f1(xx_, yy_)
lev = np.linspace(0, 20, 21)

# initialize gradient descent at (0,0), 30 iterations, ùõæ=0.01
vv_init = np.array([10, 10])
nums_iter = 100
valuess = np.zeros([nums_iter, 2])
valuess[0,:] = vv_init
vv = vv_init
gammaa = 0.03

# gradient descent algorithm
for i in range(1,nums_iter):
  vv = vv - gammaa * func_grad(v[0], v[1])
  valuess[i,:] = vv

# plot
plt.contour(xx_, yy_, zz_, levels = lev)
plt.plot(valuess[:,0], valuess[:,1], 'r-')
plt.plot(valuess[:,0], valuess[:,1], 'bo')
grad_normm = LA.norm(func_grad(v[0], v[1]))
title = "gamma %0.02f | final grad %0.3f" % (gammaa, grad_normm)
plt.title(title)
plt.show()
print('function value, gradient, and 2-norm of the gradient \n')
# ******TODO - FIXME(?)




# 3.(c) - Run any variant gradient descent on f2 with starting point 
#         (x, y) = (0, 2), T = 100 steps
# ******TODO - FIXME(?)